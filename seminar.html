<!DOCTYPE html>
<html>

<head>
    <title>VLSS Seminar Series (2021)</title>
    <link href='https://fonts.googleapis.com/css?family=Vollkorn' rel='stylesheet' type='text/css'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-7551853-2', 'auto');
  ga('send', 'pageview');

</script>
    <style>
      body {
      font-family: 'Vollkorn', sans-serif;
      }

      .subheading {
      font-size: 120%;
      }

      h2 {
      clear: left;
      margin-top: 1em;
      }
      
      h3 {
      clear: left;
      margin-top: 1em;
      }
      
      img {
      float: left;
      height: 12em;
      margin-right: 1em;
      margin-bottom: 3em;
      }

      p {
      margin: 0.5em;
      }

      nav {
      margin-top: 3em;
      margin-bottom: 1em;
      }
        
        a:visited {color:#006633}
        a:link {color:#006633}
        a:hover {color: #FF0000}
        
        h1,h2,h3{
            color:#006633
        }

    </style>
  </head>
  
  <body>
    <h1>VILSS - Seminar Series</h1>

    <div class="subheading">
      <img src="Dima2019_s.jpg" />
      Organised by Dima Damen, 
      <a href="http://www.cs.bris.ac.uk">Department of Computer Science</a>,<br/>
      <a href="http://www.bris.ac.uk/vi-lab/">Computer Vision Group</a>,
      <a href="http://www.bristol.ac.uk">University of Bristol</a>
    </div>

      
      <h2>Upcoming Seminars</h2>
      <table>
          <tr><td  style="vertical-align:top"><b>Date and Time (UK Time)</b></td> <td  style="vertical-align:top"><b>Speaker</b></td> <td  style="vertical-align:top"><b>Title</b></td> <td width="40%"><b>Abstract</b></td> <td  style="vertical-align:top"><b>ZoomLink</b></td> </tr>


          <tr><td  style="vertical-align:top">18 Jan 2022 2pm</td> <td  style="vertical-align:top"><a href="http://bryanplummer.com/">Bryan Plummer </a></td> <td  style="vertical-align:top">Vision-language reasoning from coarse categories to fine-grained tokens using sparsely annotated and loosely aligned multimodal data</td> <td  style="vertical-align:top">A key challenge in training more discriminative vision-language models is the lack of fully supervised datasets. Only a few language annotations are made for images or videos of the nearly infinite variations that may be applicable, and many annotations may be missing or only loosely associated with their visual counterparts.  In this talk I will be discussing how we can learn from these datasets, when and where we can know that language annotations are mutually exclusive to sample better negatives to create more discriminative models during training.  I will also introduce novel cross-modal attention mechanisms that enables effective optimization of a contrastive loss during training.  I will close out by briefly discussing some challenges in training large multimodal problems, especially as model size continues to grow.</td> <td  style="vertical-align:top"><a href="https://bristol-ac-uk.zoom.us/j/99469549594?pwd=WC8rYWJHSmY0NTRaN3JDMkZrK0JMQT09">Join Zoom</a></td></tr>
          
          <tr><td  style="vertical-align:top">8 Feb 2022 2pm</td> <td  style="vertical-align:top"><a href="https://arnike.github.io">Nikita Araslanov</a></td> <td  style="vertical-align:top">TBD</td> <td  style="vertical-align:top">TBD.</td> <td  style="vertical-align:top"><a href="https://bristol-ac-uk.zoom.us/j/92722444329">Join Zoom</a></td></tr>
          <tr><td  style="vertical-align:top">1 Mar 2022 TBC</td> <td  style="vertical-align:top"><a href="https://likojack.bitbucket.io/">Kejie Li</a></td> <td  style="vertical-align:top">TBD</td> <td  style="vertical-align:top">TBD.</td> <td  style="vertical-align:top"><a href="https://bristol-ac-uk.zoom.us/j/92722444329">Join Zoom</a></td></tr>
          
      </table>
      
      <h2>Previous Seminars</h2>
      <table>
          <tr><td  style="vertical-align:top"><b>Date and Time (UK Time)</b></td> <td  style="vertical-align:top"><b>Speaker</b></td> <td  style="vertical-align:top"><b>Title</b></td> <td width="40%"><b>Abstract</b></td> <td  style="vertical-align:top"><b>ZoomLink</b></td> </tr>
           <tr><td  style="vertical-align:top">14 Dec 2021 2pm</td> <td  style="vertical-align:top"><a href="https://rpand002.github.io">Rameswar Panda</a></td> <td  style="vertical-align:top">Dynamic Neural Networks for Efficient Video Inference</td> <td  style="vertical-align:top">Most existing deep neural networks for video understanding rely on one-size-fits-all models, where the exact same fixed set of features are extracted for all inputs or configurations, no matter their complexity. In contrast, humans dynamically allocate time and scrutiny for perception tasks - for example, a single glimpse is sufficient to recognize most simple actions (e.g., “Sleeping”), whereas more time and attention is required to clearly understand complex ones (e.g., “Pulling two ends of something so that it gets stretched”). In this talk, I will discuss some of our recent works on dynamic neural networks, specifically designed for efficient video understanding, which can adaptively adjust computation depending on the input videos. First, I will present a method that learns to select optimal precision conditioned on the input, while taking both accuracy and efficiency into account in recognizing complex actions. Second, I will show how a similar dynamic approach can be extended to make multimodal video recognition more computationally efficient. Finally, I will conclude the talk discussing other ongoing work on efficient vision transformers and few open research problems in dynamic neural networks.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
          <tr><td  style="vertical-align:top">30 Nov 2021 2pm</td> <td  style="vertical-align:top"><a href="https://cemse.kaust.edu.sa/ece/people/person/chen-zhao">Chen Zhao</a></td> <td  style="vertical-align:top">Detecting Actions in Videos via Graph Convolutional Networks</td> <td  style="vertical-align:top">Detecting the time duration when an action happens in a video is an important and fundamental task in video understanding. It is the key problem for various applications, such as extracting highlights in sports and identifying anomaly behaviors in surveillance videos, and it is also an important subtask for other tasks such as video language grounding and video captioning. These years have witnessed its rapid progress in performance as various methods are proposed based on convolutional neural networks (CNNs). However, CNNs face some limitations when exploring the properties of videos. For example, correlations between non-consecutive frames can not be directly utilized; large variations in action temporal duration are not effectively handled. To address these challenges, we introduce graph convolutional networks (GCNs), which were previously mostly used for non-Euclidean data,  to the task of temporal action detection. In this talk, I will present several methods to model video data as graphs, and to aggregate information from different parts of the video data via graph convolution. I will demonstrate how GCNs effectively model correlations between long-distance frames and lead to better detection performance, and show how GCNs enable establishing multi-scale correlations and benefit short action detection. I will also discuss the potential of our GCN models to apply to different tasks in video understanding.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
          <tr><td  style="vertical-align:top">19 Oct 2021 2pm</td> <td  style="vertical-align:top"><a href="https://cihangxie.github.io/">Cihang Xie</a></td> <td  style="vertical-align:top">Towards Robust Representation Learning and Beyond </td> <td  style="vertical-align:top">Deep learning has transformed computer vision in the past few years. As fueled by powerful computational resources and massive amounts of data, deep networks achieve compelling, sometimes even superhuman, performance on a wide range of visual benchmarks. Nonetheless, these success stories come with bitterness---deep networks are vulnerable to adversarial examples. The existence of adversarial examples reveals that the computations performed by the current deep networks are dramatically different from those by human brains, and, on the other hand, provides opportunities for understanding and improving these models. In this talk, I will first show that the vulnerability of deep networks is a much more severe issue than we thought---the threats from adversarial examples are ubiquitous and catastrophic. Then I will discuss how to equip deep networks with robust representations for defending against adversarial examples. We approach the solution from the perspective of neural architecture design, and show incorporating architectural elements like feature-level denoisers or smooth activation functions can effectively boost model robustness. The last part of this talk will rethink the value of adversarial examples. Rather than treating adversarial examples as a threat to deep networks, we take a further step on uncovering that adversarial examples can help deep networks substantially improve their generalization ability.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
                  <tr><td  style="vertical-align:top">5 Oct 2021 2pm</td> <td  style="vertical-align:top"><a href="http://people.csail.mit.edu/roudi/">Andrew Rouditchenko</a></td> <td  style="vertical-align:top">Learning Sight, Sound, and Language from Videos</td> <td  style="vertical-align:top">In this talk, I will describe our recent progress in multimodal learning from video, audio, and text. First, I will introduce a model for audio-visual learning from instructional videos which can relate spoken words and sounds to visual content. We propose a cascaded approach to learning multilingual representations by leveraging a model trained on English HowTo100M videos and applying it to Japanese cooking videos. This improves retrieval performance nearly 10x compared to training on the Japanese videos solely. Next, I will present a model for jointly learning from video, audio, and text that enforces a grouping of semantically similar instances in addition to sharing representations across different modalities. The training pipeline extends instance-level contrastive learning with a multimodal clustering step to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
        <tr><td  style="vertical-align:top">21 Sep 2021 3pm</td> <td  style="vertical-align:top"><a href="https://fabiancaba.com/">Fabian Caba Heilbron</a></td> <td  style="vertical-align:top">Multimodal Learning for Creative Video Applications</td> <td  style="vertical-align:top">Watching and creating videos is a multimodal experience. To understand video, one needs to reason about the movements on screen, the meaning of a speech, and the sound of objects. In this talk, Fabian will discuss two recent works addressing inherently multimodal video problems. First, he will present novel architectures for speaker identification in videos that leverage Spatio-temporal audiovisual context. Then, he will share his quest and latest works on learning the anatomy of video editing. To conclude, he will open up the discussion with exciting frontiers in this rapidly evolving research space.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
          
          <tr><td  style="vertical-align:top">7 Sep 2021</td> <td  style="vertical-align:top"><a href="http://users.cecs.anu.edu.au/~cherian/">Anoop Cherian</a></td> <td  style="vertical-align:top">InSeGAN: An Unsupervised Approach to Identical Instance Segmentation</td> <td  style="vertical-align:top">Identifying (nearly) identical instances of objects is a problem that is ubiquitous in daily life. For example, when taking a paperclip from a container, choosing an apple from a box, or removing a book from a library shelf, humans subconsciously solve this problem because we have an understanding of what the individual instances are. However, when robots are deployed for such a picking task, they need to be able to identify the instances for planning their grasp and approach. In many of these scenarios, the robot's owners often have no access to a 3D model of the object to be picked, and annotating individual instances for training can be costly, inconvenient, and unscalable. However, they may have access to collections of unlabeled images each containing multiple instances of the object, such as for example, just shaking the paperclip container. In this talk, I will introduce InSeGAN -- an unsupervised algorithm based on a 3D generative adversarial network (GAN) for segmenting instances of identical rigid objects in depth images. InSeGAN is an analysis-by-synthesis approach in which we design a novel GAN architecture to synthesize multiple-instance depth images with independent control over each instance, and which at test time can be reconfigured to produce instance segmentations for real-world depth images. To empirically validate the benefits of InSeGAN, I will showcase experiments on several synthetic and robot-collected-real-world datasets; our results outperforming several classical and very recent deep learning methods by large margins.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
          
          <tr><td  style="vertical-align:top">8 June 2021</td> <td  style="vertical-align:top"><a href="http://zhegan27.github.io/">Zhe Gan</a></td> <td  style="vertical-align:top">Recent Advances in Vision-Language Pre-training</td> <td  style="vertical-align:top">With the advent of models such as OpenAI CLIP and DALL-E, transformer-based vision-language pre-training has become an increasingly hot research topic. In this talk, I will share some of our recent work in this field that is published in NeurIPS 2020, ECCV 2020 and EMNLP 2020. Specifically, I will answer the following questions. First, how to perform vision-language pre-training? Second, how to understand what has been learned in the pre-trained models? Third, how to enhance the performance of pre-trained models via adversarial training? And finally, how can we extend image-text pre-training to video-text pre-training? Accordingly, I will present UNITER, VALUE, VILLA and HERO to answer these four questions. At last, I will also briefly discuss the challenges and future directions for vision-language pre-training.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
          
          <tr><td  style="vertical-align:top">18 May 2021</td> <td  style="vertical-align:top"><a href="https://i.cs.hku.hk/~kykwong/">Kenneth Wong</a></td> <td  style="vertical-align:top"> Deep Photometric  Stereo for Non-Lambertian Surfaces</td> <td  style="vertical-align:top">In this talk, we will introduce our recently proposed deep neural networks for solving the photometric stereo problem for non-Lambertian surfaces. Traditional approaches often adopt simplified reflectance models and constrained setups to make the problem more tractable, but this greatly hinders their applications on real-world objects. We propose a deep fully convolutional network, named PS-FCN, that predicts a normal map of an  object from an arbitrary number of images captured under different light directions. Compared with other learning based methods, PS-FCN does not depend on a pre-defined set of light directions and can handle multiple images in an order-agnostic manner. To tackle uncalibrated light directions, we propose a deep convolutional neural network, named LCNet, that predicts per-image light direction and intensity from both local and global features extracted from all input images. We analyze the features learned by LCNet and find they resemble attached shadows, shadings, and specular highlights, which are known to provide clues in resolving  GBR ambiguity. Based on this insight, we propose a guided calibration network, named GCNet, that explicitly leverages object shape and shading information for improved lighting estimation. Experiments on synthetic and real data will be presented to demonstrate the effectiveness of our proposed networks.</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">4 May 2021</td> <td  style="vertical-align:top"><a href="https://www.cs.cmu.edu/~aayushb/">Aayush Bansal</a></td> <td  style="vertical-align:top">Unsupervised Exemplar Representations: Beyond Task-based Optimization</td> <td  style="vertical-align:top">We are tuned to think in terms of "tasks" in computer vision, graphics, machine learning, and robotics. Given a problem, we collect a large amount of domain-specific and task-specific data. We then get appropriate human annotations on massive datasets to enable highly-tuned optimizations based on supervised learning. I believe this setup is prohibitive and has hindered progress in the community. In this talk, I will present two unsupervised approaches that build on a simple autoencoder. In the first part of the talk, I will talk about Exemplar Autoencoders trained on an individual's voice and yet generalizes for unknown voices in different languages. I will demonstrate its application as an assistive tool for speech-impaired and multi-lingual translation. In the second part of the talk, I will talk about Video-Specific Autoencoders trained on random frames of a video without temporal information via a simple reconstruction loss. The learned representation allows us to do a wide variety of video analytic tasks such as (but not limited to) spatial and temporal super-resolution, spatial and temporal editing, video textures, average video exploration, and correspondence estimation within and across videos. Neither approach optimizes for the end task and still competes with state-of-the-art supervised methods that do task-specific optimization.</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">20 April 2021</td> <td  style="vertical-align:top"><a href="http://www.columbia.edu/~zs2262/">Mike Shou</a></td> <td  style="vertical-align:top">Generic Event Boundary Detection: A Benchmark for Event Segmentation</td> <td  style="vertical-align:top">n this talk, I will present a novel task together with a new benchmark for detecting generic, taxonomy-free event boundaries that segment a whole video into chunks. Conventional work in temporal video segmentation and action detection focuses on localizing pre-defined action categories and thus does not scale to generic videos. Cognitive Science has known since last century that humans consistently segment videos into meaningful temporal chunks. This segmentation happens naturally, without pre-defined event categories and without being explicitly asked to do so. Here, we repeat these cognitive experiments on mainstream CV datasets; with our novel annotation guideline which addresses the complexities of taxonomy-free event boundary annotation, we introduce the task of Generic Event Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our Kinetics-GEBD has the largest number of boundaries (e.g. 32x of ActivityNet, 8x of EPIC-Kitchens-100) which are in-the-wild, open-vocabulary, cover generic event change, and respect human perception diversity. We view GEBD as an important stepping stone towards understanding the video as a whole, and believe it has been previously neglected due to a lack of proper task definition and annotations. Through experiment and human study we demonstrate the value of the annotations. Further, we benchmark supervised and un-supervised GEBD approaches on the TAPOS dataset and our Kinetics-GEBD, together with method design explorations that suggest future directions. We release our annotations and baseline codes at CVPR'21 LOVEU Challenge: https://sites.google.com/view/loveucvpr21</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">23 Mar 2021</td> <td  style="vertical-align:top"><a href="http://renaud-detry.net">Renaud Detry</a></td> <td  style="vertical-align:top">Autonomous robot manipulation for planetary and terrestrial applications.</td> <td  style="vertical-align:top">In this talk, I will first discuss the experimental validation of autonomous robot behaviors that support the exploration of Mars' surface, lava tubes on Mars and the Moon, icy bodies and ocean worlds, and operations on orbit around the Earth. I will frame the presentation with the following questions: What new insights or limitations arise when applying algorithms to real-world data as opposed to benchmark datasets or simulations? How can we address the limitations of real-world environments—e.g., noisy or sparse data, non-i.i.d. sampling, etc.? What challenges exist at the frontiers of robotic exploration of unstructured and extreme environments? I will discuss our approach to validating autonomous machine-vision capabilities for the notional Mars Sample Return campaign, for autonomously navigating lava tubes, and for autonomously assembling modular structures on orbit. The talk will highlight the thought process that drove the decomposition of a validation need into a collection of tests conducted on off-the-shelf datasets, custom/application-specific datasets, and simulated or physical robot hardware, where each test addressed a different range of experimental parameters for sensing/actuation fidelity, breadth of environmental conditions, and breadth of jointly-tested robot functions. Next, I will present a task-oriented grasp model, that en- codes grasps that are configurationally compatible with a given task. The model consists of two independent agents: First, a geometric grasp model that computes, from a depth image, a distribution of 6D grasp poses for which the shape of the gripper matches the shape of the underlying surface. The model relies on a dictionary of geometric object parts annotated with workable gripper poses and preshape parameters. It is learned from experience via kinesthetic teaching. The second agent is a CNN-based semantic model that identifies grasp- suitable regions in a depth image, i.e., regions where a grasp will not impede the execution of the task. The semantic model allows us to encode relationships such as “grasp from the handle.” A key element of this work is to use a deep network to integrate contextual task cues, and defer the structured-output problem of gripper pose computation to an explicit (learned) geometric model. Jointly, these two models generate grasps that are mechanically fit, and that grip on the object in a way that enables the intended task.</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">16 Feb 2021</td> <td  style="vertical-align:top"><a href="https://www.dimtzionas.com/">Dimitris Tzionas</a></td> <td  style="vertical-align:top">From Interacting Hands to Expressive and Interacting Humans</td> <td  style="vertical-align:top">A long-term goal of computer vision and artificial intelligence is to develop human-centred AI that perceives humans in their environments and helps them accomplish their tasks. For this, we need holistic 3D scene understanding, namely modelling how people and objects look, estimating their 3D shape and pose, and inferring their semantics and spatial relationships. For humans and animals this perceptual capability seems effortless, however, endowing computers with similar capabilities has proven to be hard. Fundamentally, the problem involves observing a scene through cameras, and inferring the configuration of humans and objects from images. Challenges exist at all levels of abstraction, from the ill-posed 3D inference from noisy 2D images, to the semantic interpretation of it. The talk will discuss several projects (IJCV’16, TOG’17, CVPR’19, ICCV’19, ECCV’20) that attempt to understand, formalize, and model increasingly complex cases of human-object interactions. These cases range from interacting hands to expressive and interacting whole-body humans. More specifically, the talk will present novel statistical models of the human hand and the whole body, and the usage of these models (1) to efficiently regularize 3D reconstruction from monocular 2D images and eventually (2) to build statistical models of interactions. The presented models are freely available for research purposes.</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">2 Feb 2021 3pm</td> <td  style="vertical-align:top"><a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a></td> <td  style="vertical-align:top">3D Photography and Videography</td> <td  style="vertical-align:top"> Images and videos allow us to capture and share memorable moments of our lives. However, 2D images and videos appear flat due to the lack of depth perception. In this talk, I will present our recent efforts to overcome these limitations. Specifically, I will cover our recent work for creating compelling 3D photography, estimating consistent video depth for advanced video-based visual effects, and free-viewpoint videos. I will conclude the talk with some ongoing research and research challenges ahead. </td> <td  style="vertical-align:top">EXPIRED</td></tr>
          
          
      </table>
    </body>
      


</html>
